{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13215644,"sourceType":"datasetVersion","datasetId":8376470}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv(\"/kaggle/input/chatbot/train.csv\")\nval = pd.read_csv(\"/kaggle/input/chatbot/validation.csv\")\ntest = pd.read_csv(\"/kaggle/input/chatbot/test.csv\")\n\nprint(\"Train size:\", len(train))\nprint(\"Validation size:\", len(val))\nprint(\"Test size:\", len(test))\nprint(train.head())\nprint(train.columns)\nprint(train.head())\n\nimport ast\n\ndef preprocess_dialogue(dialog_str):\n    # Convert string representation of list â†’ actual Python list\n    turns = ast.literal_eval(dialog_str)\n    # Strip whitespace and remove empty turns\n    turns = [t.strip() for t in turns if t.strip()]\n    # Join turns with <eos> token\n    return \" <eos> \".join(turns)\n\n# Apply to train, validation, and test sets\ntrain[\"processed\"] = train[\"dialog\"].apply(preprocess_dialogue)\nval[\"processed\"] = val[\"dialog\"].apply(preprocess_dialogue)\ntest[\"processed\"] = test[\"dialog\"].apply(preprocess_dialogue)\n\n# Preview\nprint(train[\"processed\"].head())\n\nfrom transformers import AutoTokenizer\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n\n# Set pad_token to eos_token\ntokenizer.pad_token = tokenizer.eos_token\n\n# Now tokenize\ntrain_encodings = tokenizer(list(train[\"processed\"]),\n                            truncation=True,\n                            padding=True,\n                            max_length=128)\nval_encodings = tokenizer(list(val[\"processed\"]),\n                          truncation=True,\n                          padding=True,\n                          max_length=128)\ntest_encodings = tokenizer(list(test[\"processed\"]),\n                           truncation=True,\n                           padding=True,\n                           max_length=128)\n\n# Check a sample\nprint(train_encodings[\"input_ids\"][0])\n\nimport torch\n\nclass ChatDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.input_ids = encodings[\"input_ids\"]\n        self.attention_mask = encodings[\"attention_mask\"]\n        \n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.input_ids[idx]),\n            \"attention_mask\": torch.tensor(self.attention_mask[idx])\n        }\n\n# Create datasets\ntrain_dataset = ChatDataset(train_encodings)\nval_dataset = ChatDataset(val_encodings)\ntest_dataset = ChatDataset(test_encodings)\n\nfrom torch.utils.data import DataLoader\n\n# Batch size\nbatch_size = 2  # you can increase if GPU allows\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Check a batch\nbatch = next(iter(train_loader))\nprint(batch[\"input_ids\"])\nprint(batch[\"attention_mask\"])\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load DialoGPT tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n\n# Set pad token (important for batching)\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = model.config.eos_token_id\n\nfrom torch.optim import AdamW\n\n# Optimizer for model parameters\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nimport torch\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n\nfrom torch.nn import CrossEntropyLoss\n\n# Number of epochs\nepochs = 2  # start small to test\nloss_fn = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)  # ignore padding\n\nfor epoch in range(epochs):\n    model.train()  # set model to training mode\n    total_loss = 0\n\n    for batch in train_loader:\n        # Move batch to device (GPU/CPU)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        # Reset gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n        loss = outputs.loss\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} | Average Loss: {avg_loss}\")\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T10:56:27.198084Z","iopub.execute_input":"2025-09-30T10:56:27.198859Z","iopub.status.idle":"2025-09-30T10:57:09.239959Z","shell.execute_reply.started":"2025-09-30T10:56:27.198832Z","shell.execute_reply":"2025-09-30T10:57:09.238948Z"}},"outputs":[{"name":"stdout","text":"Train size: 11118\nValidation size: 1000\nTest size: 1000\n                                              dialog                    act  \\\n0  ['Say , Jim , how about going for a few beers ...  [3 4 2 2 2 3 4 1 3 4]   \n1  ['Can you do push-ups ? '\\n \" Of course I can ...          [2 1 2 2 1 1]   \n2  ['Can you study with the radio on ? '\\n ' No ,...            [2 1 2 1 1]   \n3  ['Are you all right ? '\\n ' I will be all righ...              [2 1 1 1]   \n4  ['Hey John , nice skates . Are they new ? '\\n ...    [2 1 2 1 1 2 1 3 4]   \n\n                 emotion  \n0  [0 0 0 0 0 0 4 4 4 4]  \n1          [0 0 6 0 0 0]  \n2            [0 0 0 0 0]  \n3              [0 0 0 0]  \n4    [0 0 0 0 0 6 0 6 0]  \nIndex(['dialog', 'act', 'emotion'], dtype='object')\n                                              dialog                    act  \\\n0  ['Say , Jim , how about going for a few beers ...  [3 4 2 2 2 3 4 1 3 4]   \n1  ['Can you do push-ups ? '\\n \" Of course I can ...          [2 1 2 2 1 1]   \n2  ['Can you study with the radio on ? '\\n ' No ,...            [2 1 2 1 1]   \n3  ['Are you all right ? '\\n ' I will be all righ...              [2 1 1 1]   \n4  ['Hey John , nice skates . Are they new ? '\\n ...    [2 1 2 1 1 2 1 3 4]   \n\n                 emotion  \n0  [0 0 0 0 0 0 4 4 4 4]  \n1          [0 0 6 0 0 0]  \n2            [0 0 0 0 0]  \n3              [0 0 0 0]  \n4    [0 0 0 0 0 6 0 6 0]  \n0    Say , Jim , how about going for a few beers af...\n1    Can you do push-ups ?  Of course I can . It's ...\n2    Can you study with the radio on ?  No , I list...\n3    Are you all right ?  I will be all right soon ...\n4    Hey John , nice skates . Are they new ?  Yeah ...\nName: processed, dtype: object\n[25515, 837, 5395, 837, 703, 546, 1016, 329, 257, 1178, 16800, 706, 8073, 5633, 220, 921, 760, 326, 318, 29850, 475, 318, 1107, 407, 922, 329, 674, 13547, 764, 220, 1867, 466, 345, 1612, 5633, 632, 481, 1037, 514, 284, 8960, 764, 220, 2141, 345, 1107, 892, 523, 5633, 314, 836, 470, 764, 632, 481, 655, 787, 514, 3735, 290, 719, 14397, 764, 11436, 938, 640, 5633, 220, 314, 4724, 345, 389, 826, 13, 1537, 644, 2236, 356, 466, 5633, 314, 836, 470, 1254, 588, 5586, 379, 1363, 764, 220, 314, 1950, 257, 2513, 625, 284, 262, 11550, 810, 356, 460, 711, 33041, 506, 290, 1826, 617, 286, 674, 2460, 764, 220, 1320, 338, 257, 922, 2126, 764, 314, 3285, 5335, 290, 25737, 1690, 467, 612, 284, 711]\ntensor([[11980,   345,  5201,   534,   670,  5633,   220,  1892,   780,   314,\n           564,   247,   285,  5291,   281,  4151,   319,   262,  5156,   764,\n           220,  6350,   318,   262,  5156,   564,   247,   264,  2802,  5633,\n           220,  1375,   318,   379,   262, 26454, 34624,   764,  1375,   531,\n           673,   561,   307,   736,   287,   546,  2063,   281,  1711,   764,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n        [10248,  3329,   764,   314,   564,   247,   285, 18258, 26463,   837,\n           422,  2807, 41420,    57,  5834,   764,   775,   423,   281, 12557,\n           351,  1770,   764,  5658,   764,   220, 19134,   284,  9738,  5834,\n           764,   775,   423,   587, 12451,   345,   837,  3387,   423,   257,\n          5852,   764,   314,   481,   869,   683,   764,   220,  6952,   345,\n           764,   220,  1770,   764,  5658,   481,   307,   994,   845,  2582,\n           764,   554,   262, 14324,   837,   743,   314,  1265,   477,   286,\n           345,   284,  1051,   287,   523,   326,   314,   460,  2071,   534,\n          8318,  5633,   220, 10358,  1123,   286,   514,  3551, 13869,  5633,\n           220,  3363,   837,  3387,   764,  4222,  3601,   534,  1438,   290,\n          1664,  1438,   837,   290,   262,  1048,   345,   389,  1016,   284,\n           766,   764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1974885768.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"# Step 1: Save the fine-tuned model and tokenizer\n\n# Save the model\nmodel.save_pretrained(\"my_finetuned_dialoGPT\")\n\n# Save the tokenizer\ntokenizer.save_pretrained(\"my_finetuned_dialoGPT\")\n\nprint(\"Model and tokenizer saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T10:58:21.458995Z","iopub.execute_input":"2025-09-30T10:58:21.459383Z","iopub.status.idle":"2025-09-30T10:58:22.403361Z","shell.execute_reply.started":"2025-09-30T10:58:21.459357Z","shell.execute_reply":"2025-09-30T10:58:22.402495Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer saved successfully!\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!pip install ipywidgets\nfrom IPython.display import display\nimport ipywidgets as widgets\nimport torch\n\n# Assume your model and tokenizer are already loaded\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# model.to(device)\n\ninput_box = widgets.Text(\n    value='',\n    placeholder='Type your message here...',\n    description='You:',\n    disabled=False\n)\n\noutput_area = widgets.Output()\n\ndef on_submit(sender):\n    user_msg = input_box.value.strip()\n    if not user_msg:\n        return\n    input_box.value = ''  # clear input box\n\n    # Encode user input only (no memory)\n    input_ids = tokenizer.encode(user_msg + tokenizer.eos_token, return_tensors=\"pt\").to(device)\n\n    # Generate a short response\n    output_ids = model.generate(\n        input_ids,\n        max_length=20,                  # short reply\n        pad_token_id=tokenizer.eos_token_id,\n        do_sample=True,\n        top_k=30,\n        top_p=0.9,\n        temperature=0.5,\n        early_stopping=True,\n        no_repeat_ngram_size=2\n    )\n\n    response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n\n    with output_area:\n        print(\"You:\", user_msg)\n        print(\"Chatbot:\", response)\n        print()\n\ninput_box.on_submit(on_submit)\ndisplay(input_box, output_area)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T11:33:45.610384Z","iopub.execute_input":"2025-09-30T11:33:45.611024Z","iopub.status.idle":"2025-09-30T11:33:48.782497Z","shell.execute_reply.started":"2025-09-30T11:33:45.610981Z","shell.execute_reply":"2025-09-30T11:33:48.781584Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (8.1.5)\nRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.2)\nRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\nRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\nRequirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (4.0.14)\nRequirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (75.2.0)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\nRequirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\nRequirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/4046682551.py:48: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n  input_box.on_submit(on_submit)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Text(value='', description='You:', placeholder='Type your message here...')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b22b7ec63c164a6fb9322a2bbe800ff7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5a683f55776444f90674407778a9770"}},"metadata":{}}],"execution_count":40}]}